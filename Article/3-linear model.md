# 0x00 介绍
在一二章节，我们已经把机器学习的一些基础的概念大致的过了一遍。接下来我们就要深入各个机器学习模型，理解各个机器学习模型的原理和应用场景。

在这一节我们讲一下最基础的机器学习模型-线性模型
# 0x01 正文
### 基本形式
给定一个n个特征描述的样本 **X** = [x1, x2, x3, ..., xn], 其中xi 是**X**的第i个属性上的值。线性模型试图学的一个通过属性的线性组合来进行预测的函数


<!--more-->


![_6@HMY[B[F14JY_6AI`AFI6.png][1]

> 如果只有一个特征值。那么预测函数就是 **f = w1*x +b** ，也就是我们十分熟悉的老朋友 - 一次函数。 我们可以调整w1（斜率），b（偏置），来调整一次函数对数据的拟合程度，尽可能的让点都在一条直线上。
 
线性模型形式简单，易于建模。许多功能更为强大的非线性模型可在线性模型的基础上通过引入层级结构或高维映射而得。此外，由于**w**直观表达了各个属性在预测中的重要性（在这里可认为是**权重**），因此线性模型右很好的可解释性。

> 例如  一台电脑的性能 = 0.5*cpu + 0.3*gpu + 0.2*screen .....  当然这只是举个例子，但是我们可以很清楚的看出各个属性所包含的权重，推出cpu最重要等结论

### 优化算法
那么我们如何调整各个参数呢？那我们如何在数据中学到东西呢？如果我们将一个初始的模型，在数据中"学到"各个参数，变成最拟合数据的模型（误差最小的模型）。那么这就是一个最简单的机器学习模型。
首先，我们在前面一二章节学到，如果一个模型更加的拟合数据，也就是使得模型误差最小（有各种衡量误差的方法，这里我们使用**均方误差**），那么我们的任务就是试图让**均方误差最小化**。
![YKDYO4B48G46`2}SPM@IUM1.png][2]
（均方误差有非常好的几何意义，对应了常用的,我们熟悉的**欧几里得距离（欧氏距离）**，基于均方误差最小化来进行模型求解的方法称为“最小二乘法”）
这个函数最小化的过程，称为最小二乘**"参数估计"**.
回想一下，我们以前有没有处理过关于最小化的问题，也许换个说法你就会记起来。没错，就是求二次函数的最低点，当x²系数为正时，我们可以求得函数最小值。那么在二次函数最低点时候，我们可以知道函数在那一点是**平的**，就也就导数为0.一般我们在求解二次函数最值问题时候，都会令导数为0。（详细的就回顾下数学知识）.
回到我们的问题，对于均方误差求解最优化问题，我们也是使用相同的方法-> **令偏导数为0**.因为我们的模型中不止一个参数。所以要对各个参数求偏导数。
![][3]

![A9%T0GEV))4RI5[6@)`L9E6.png][4]
更一般的，我们可以表示成矩阵的形式

![][5]
你现在就掌握最为基础的优化算法了！


### 线性回归
给定数据集D={(x1,y1),(x2,y2),(x3,y3),....} 其中xi = (xi1,xi2,...xin)。线性回归试图学得一个线性模型尽可能准确地预测实值输出标记。
> 前面我们说的都是线性模型。也就是函数的图像都是直线的。那么，如果有时候数据是呈曲线的，那么我们的线性模型该怎么去拟合呢？这里映入一个映射的概念，其实简单来说就是将线性模型当作自变量再嵌入一层函数中，简单来说，就是**复合函数**.
![797549QX5FY)K7~EZ_0HKGS.png][6]
这样，我们的函数就是一个非线性模型了
![][7]

### 对数几率回归
上面我们都是讨论用线性模型进行**回归学习**，那么可以用线性模型进行**分类学习**吗？。答案是可以的
![][8]
图中的函数就是对数几率函数。
![][9]
线性回归模型的预测结果去逼近真实标记的对数几率，因此称为**对数几率回归**。虽然称为回归，但其实是分类学习方法.
同时它也有线性模型所具备的优点，比如直接对分类可能性建模；不是仅预测出类别，而是得到近似概率预测，对许多需利用概率辅助决策的任务很有用；此外，对率函数数学性质很好，也就是很方便求取最优解.

下面我们来看看如何求解w,b
 ![][10]
以后我们会讲什么是梯度下降算法，牛顿法等优化算法.
我们就初步了解到了对数几率回归（逻辑斯蒂回归）的原理了.

### 类别不平衡问题
机器学习模型性能表现的好坏，和数据集的关系很大。一个好的数据集的训练出来的模型要比不好的表现好很多。这一节我们讲讲训练集中类别不平衡的问题。
类别不平衡指的是分类任务中不同类别的训练样例数目差别很大的情况。、

> 假如训练集中有900个正例，100个反例，那么如果模型全部输出正例，得到的正确率也有90%。所以我们可以看出从中的问题。

那么我们是怎么解决的呢？
一个基本策略 --> **再缩放**
![][11]
简单来说，就是阈值进行缩放。假如在我们的对数几率回归中，预测值y大于0.5则判别正类，这是基于训练集中类别数目相同情况下设置的阈值。结合上题语境，我们可以把阈值设得高一些，比如0.9，这样就可以达到我们的目的.

还有其他方法
+ 欠采样
    + 直接去掉训练集的多的部分，使得两边数目接近
    + 随机丢弃样本，可能丢失重要信息
+ 过采样
    + 重复的添加训练集中确实的样本
    + 可能导致严重的过拟合   


# 0x02 总结
本章讲了：
+ 线性模型，广义线性模型
+ 对数几率回归
+ 类别不平衡

下一节，我们想要讲另一种用线性模型.而这种线性模型具有很好的理解性。是一种极其强大的机器学习模型的基础


  [1]: http://120.77.246.8/usr/uploads/2018/03/3070932373.png
  [2]: http://120.77.246.8/usr/uploads/2018/03/318073612.png
  [3]: http://120.77.246.8/usr/uploads/2018/03/1817256087.png
  [4]: http://120.77.246.8/usr/uploads/2018/03/2190218999.png
  [5]: http://120.77.246.8/usr/uploads/2018/03/1817256087.png
  [6]: http://120.77.246.8/usr/uploads/2018/03/1196312240.png
  [7]: http://120.77.246.8/usr/uploads/2018/03/3070932373.png
  [8]: http://120.77.246.8/usr/uploads/2018/03/1270589053.png
  [9]: http://120.77.246.8/usr/uploads/2018/03/3581907172.png
  [10]: http://120.77.246.8/usr/uploads/2018/03/716789109.png
  [11]: http://120.77.246.8/usr/uploads/2018/03/1235745335.png
