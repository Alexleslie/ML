# 0x00 介绍
感知机(perceptron)是**二类分类**的线性分类模型，输入特征向量，输出为实例的类别，取+1或者-1二值。
感知机对应于输入空间将实例划分为正负两类的**分离超平面**，属于判别模型。
感知机学习旨在求出将训练数据进行线性划分的分离超平面

感知机的前提是训练数据线性可分。举个例子，假如有两堆沙子（正类和负类）互不混合，感知机做的就是从中划出一条线（分离超平面）分离这两堆沙子（完成分类）。


<!--more-->


> **超平面**有点抽象，但是在低维度下容易理解。假设有两个点，从中画出一条线分离这两个点，那么这条线称为超平面。假如有两条平行的线（可以想象正方体其中的两条边），那么可以想象有一个面会把这条线分开，那么这个面也是超平面。这就是超平面形象的意思

# 0x01 正文
### 感知机模型
![][1]
其中线性方程 `w * x + b = 0 ` 就是一个超平面
![][2] 
### 感知机学习策略
#### 线性可分
如果线性可分，那么对于所有正例点 `w * x + b > 0` ，对于所有反例点 `w * x + b < 0`。
前面我们说到，感知机的前提是数据线性可分。因为如果你要求得分离超平面使得完全分开数据的话，那么数据必须不能**混在一起**，因为感知机是线性模型。想象一下，如果两堆沙子完全混在一起，那么可以找到一个面分开吗？有同学有疑问，沙堆不是立体（三维）么，那么对应的分离超平面应该是某个四维的东西才对啊.其实这当中用到了一点多维映射的知识，我们以后再说。

#### 学习策略
既然我们确定好了预测函数，接下来就是确定损失函数了。那么要定哪个函数适合呢？
感知机的损失函数是**误分类点到超平面S的总距离**
![][3]
这样，我们就定义好了损失函数了。接下来就是求解损失函数的**最优化问题**了

### 感知机学习算法
这一节，我们将会接触到我们学的第一个最优化算法。**随机梯度下降法(stochastic gradient descent)**

![F2KXQE118JVS2VBYO61_(2W.png][4]
![KYM%4HW{PJ066ZO6YH)O3KD.png][5]

> 已经可以证明,如果训练数据集是线性可分的,那么有一定存在超平面可以将训练集完全正确分开.那么我们的学习算法就一定会收敛.

算法收敛后,我们就得到了我们要的参数值,这样一个完整的感知机模型就训练好了.

# 0x02 总结
![{(P@7C4GZ5I1{RIGU5DFQ8S.png][6]

>  在这里说一下,为什么感知机具有**无穷多个解**. 还记得我们是如何定义损失函数的吗? **误分类点到超平面的距离**,所以如果训练集完全被正确分开的话(没有误分类点),损失函数值为0. 举个例子,假如有两个点,而我们有无数条直线可以把这两个点分开,感知机所做的也就是只要正确分开两个点, 其余没有做任何限制,那么我们的超平面就有无限多个,所以参数值也就有无穷多个解.那么有没有一种限制使得感知机可以得到唯一解而且效果最好呢? 答案是有的. 而这就是后来我们会讲到的鼎鼎大名的**支持向量机(Support vector machine  SVM)**



  [1]: http://120.77.246.8/usr/uploads/2018/03/2750956294.png
  [2]: http://120.77.246.8/usr/uploads/2018/03/968876089.png
  [3]: http://120.77.246.8/usr/uploads/2018/03/2200153087.png
  [4]: http://120.77.246.8/usr/uploads/2018/04/63169670.png
  [5]: http://120.77.246.8/usr/uploads/2018/04/138494252.png
  [6]: http://120.77.246.8/usr/uploads/2018/04/1699980341.png
